- 工作中遇到了什么复杂的业务场景，解决方法？
- 工作中遇到了什么技术性的难题，怎么解决的？
- 工作中遇到了什么复杂的bug，怎么解决的？
- 工作中有没有遇到什么性能的问题，怎么解决的？（性能问题的排查流程）
- 工作中的一些有成就感的事情。
- 设计模式在业务中的使用
- 多线程在业务中的使用。线程池怎么用的，结合具体业务
- 服务响应慢，如何排查
- 慢SQL优化、JVM调优
- 方法运行时的出栈与入栈
- Spring Cloud在业务中的使用
- 看过Redis、RabbitMQ的源码吗？
- 容器/并发容器，有比较了解的吗，具体聊聊。
- 生产环境，CPU占用过高，分析及定位
- safepoint是什么
- invokedynamic指令是干什么的
- 假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如何将它们全部 找出来? 可以使用keys，但会阻塞，也可使用scan，但会重复，看业务场景。
- 线上FullGC卡顿性能优化
- 线上OOM内存溢出问题
- 平时有些思考，若请求量增加1百倍，1万倍，现有的服务架构要如何演变；

。。。。。

- 职业发展，短期、长期。学习方法。
- 如果你本身工作没有技术挑战，那么尽可能多给自己设立一些挑战，多学一些技术，多做一些尝试和实践。

。。。。。

- 微博的动态推送怎么实现
    - 推模式（消息拷贝到关注侧）和拉模式（消息保存在发起侧，需主动拉取）。
    - 普通的应该是推模式。大V才有拉模式，具体记不清拉。
- 购票系统如何避免超卖
    - 1000张票，10个实例集群，可以每个集群分120张（放入Redis），考虑到流量分布可能不均匀，为了避免少卖，每份可以比100多一点，但超卖问题也要考虑解决。
    - 异步、MQ削峰
    - Redis瓶颈：将票分开放，比如120张票，可以分成4份（不同的key：p_1，p_2，p_3，p_4），每份30张，这4份很可能就在不同的槽中，从而落在集群中不同的节点上
    - 针对单集群的120张不超卖，可以使用lua脚本，原子更新
    - 甚至可以在本地缓存中放些信息，从而减轻Redis的压力
    - 针对于全部的1000张如何避免超卖，应该是要加些最终校验逻辑的
    - 最后当没票了之后，在Nginx层可直接拦掉，不让流量再抵达服务侧
    - 分流方面：
        - 一方面可以根据地区分（京东、淘宝这些商品在不同的库存是不一样的，这样就可以分成多个独立的集群）
        - 另一方面还可以从其他纬度，比如商品的类型，甚至在多个热门商品的情况下，可以把他们分到不同的节点中去
- 给一个 10G 的文件，里面只有两行记录是一样的，如何找出（电脑内存只有 500M）
    - 分治法是解决此类问题的常用方式：内容hash取模。分成不同的小块，分治处理。
    - 部分情况下也可以使用位图（前提是能放下）
- 点赞系统设计
    - 系统设计：功能如何实现的问题
        - 以内容为纬度，保存总点赞数
        -
    - 支持高并发
        - DNS分流到不同的机房。
        - Nginx分流到不同到集群。根据用户属性分片，使得用户尽量由固定的集群处理。
        - 虽然对于持续的高并发，MQ用途不大，但还是要的
        - 多个Redis集群，每个承载一部分。定时统计总数并更新到缓存，排行之类的也定时更新。
        - 甚至可以使用本地缓存，来减轻Redis侧的压力（数据先记录本地，以用户的纬度存其对内容点赞/取消的动作，然后每30S合并并更新到Redis，然后有个任务每5分钟将各集群的汇总并把最新的点赞数更新到内容上。） 如果能保证点赞后，只能做取消操作的话，可以参考下面的做法，本地这样缓存
      ```
      用户A点赞的帖子:
      帖子1 -1 
      帖子2 1 
      帖子3 1 
      帖子4 -1

      用户B点赞的帖子:
      帖子1 1 
      帖子2 -1 
      帖子5 1 
      帖子6 -1

      用户C点赞的帖子:
      帖子3 1 
      帖子4 1 
      帖子5 -1 
      帖子7 1
      ```
      然后定时合并并去掉不变的之后。这样再更新到Redis的话，其压力会小很多
      ```
      帖子6 -1
      帖子7 1
      帖子3 2
      ```
